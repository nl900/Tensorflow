{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0daf264",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-01 09:48:30.381597: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-01 09:48:42.513642: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_3' with dtype int64 and shape [4]\n",
      "\t [[{{node Placeholder/_3}}]]\n",
      "2023-06-01 09:48:42.514187: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype string and shape [4]\n",
      "\t [[{{node Placeholder/_2}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144/291 [=============>................] - ETA: 1:04 - loss: 0.2186 - binary_accuracy: 0.9030"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 65 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260/291 [=========================>....] - ETA: 13s - loss: 0.1845 - binary_accuracy: 0.9198"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 239 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274/291 [===========================>..] - ETA: 7s - loss: 0.1806 - binary_accuracy: 0.9211"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 1153 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277/291 [===========================>..] - ETA: 6s - loss: 0.1798 - binary_accuracy: 0.9216"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 228 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291/291 [==============================] - ETA: 0s - loss: 0.1780 - binary_accuracy: 0.9227"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-01 09:50:53.522329: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int64 and shape [1]\n",
      "\t [[{{node Placeholder/_4}}]]\n",
      "2023-06-01 09:50:53.522813: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype string and shape [1]\n",
      "\t [[{{node Placeholder/_2}}]]\n",
      "Corrupt JPEG data: 2226 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291/291 [==============================] - 163s 551ms/step - loss: 0.1780 - binary_accuracy: 0.9227 - val_loss: 0.0831 - val_binary_accuracy: 0.9708\n",
      "Epoch 2/5\n",
      "291/291 [==============================] - 164s 563ms/step - loss: 0.1145 - binary_accuracy: 0.9526 - val_loss: 0.0776 - val_binary_accuracy: 0.9703\n",
      "Epoch 3/5\n",
      "291/291 [==============================] - 163s 560ms/step - loss: 0.1099 - binary_accuracy: 0.9551 - val_loss: 0.0744 - val_binary_accuracy: 0.9725\n",
      "Epoch 4/5\n",
      "291/291 [==============================] - 159s 545ms/step - loss: 0.1058 - binary_accuracy: 0.9554 - val_loss: 0.0792 - val_binary_accuracy: 0.9686\n",
      "Epoch 5/5\n",
      "291/291 [==============================] - 180s 618ms/step - loss: 0.1015 - binary_accuracy: 0.9579 - val_loss: 0.0694 - val_binary_accuracy: 0.9725\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\"\"\"\n",
    "When there's too little data for the task to train a full scale model from scratch\n",
    "1. take layers from a previously trained model\n",
    "2. freeze the layers \n",
    "3. add new trainable layers ontop of the frozen layers\n",
    "4. train the new layers on the dataset \n",
    "\"\"\"\n",
    "\n",
    "train_ds, validation_ds, test_ds = tfds.load(\n",
    "    \"cats_vs_dogs\",\n",
    "    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"], # split for training, validation and test\n",
    "    as_supervised=True,  # Include labels\n",
    ")\n",
    "\n",
    "# standardise the sizes to 150x150\n",
    "size = (150, 150)\n",
    "train_ds = train_ds.map(lambda x, y: (tf.image.resize(x, size), y))\n",
    "validation_ds = validation_ds.map(lambda x, y: (tf.image.resize(x, size), y))\n",
    "test_ds = test_ds.map(lambda x, y: (tf.image.resize(x, size), y))\n",
    "\n",
    "batch_size = 32\n",
    "train_ds = train_ds.cache().batch(batch_size).prefetch(buffer_size=10)\n",
    "validation_ds = validation_ds.cache().batch(batch_size).prefetch(buffer_size=10) # optimise loading speed\n",
    "test_ds = test_ds.cache().batch(batch_size).prefetch(buffer_size=10)\n",
    "\n",
    "# randomly flip or rotate images to improve sample diversity and reduce overfitting\n",
    "data_augmentation = keras.Sequential([layers.RandomFlip(\"horizontal\"), layers.RandomRotation(0.1),])\n",
    "\n",
    "base_model = keras.applications.Xception(\n",
    "    weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(150, 150, 3),\n",
    "    include_top=False,\n",
    ")  # Do not include the ImageNet classifier at the top.\n",
    "\n",
    "base_model.trainable = False # Freeze the base_model\n",
    "\n",
    "# Create new model on top\n",
    "inputs = keras.Input(shape=(150, 150, 3))\n",
    "x = data_augmentation(inputs)  # Apply random data augmentation\n",
    "\n",
    "# Pre-trained Xception weights requires that input be scaled\n",
    "# from (0, 255) to a range of (-1., +1.), the rescaling layer outputs: `(inputs * scale) + offset`\n",
    "scale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\n",
    "x = scale_layer(x)\n",
    "\n",
    "# The base model contains batchnorm layers. We want to keep them in inference mode\n",
    "# when we unfreeze the base model for fine-tuning, so we make sure that the\n",
    "# base_model is running in inference mode here.\n",
    "x = base_model(x, training=False)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout\n",
    "outputs = keras.layers.Dense(1)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# train\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.BinaryAccuracy()],\n",
    ")\n",
    "\n",
    "epochs = 5\n",
    "history = model.fit(train_ds, epochs=epochs, validation_data=validation_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8c6825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286/291 [============================>.] - ETA: 11s - loss: 0.0842 - binary_accuracy: 0.9658"
     ]
    }
   ],
   "source": [
    "base_model.trainable = True # unfreeze \n",
    "\n",
    "# optional fine-tuning: train the entire model with a low learning rate\n",
    "# base model still running in inference mode since created with training=False \n",
    "# the batch normalization layers inside won't update\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-5),  # Low learning rate\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.BinaryAccuracy()],\n",
    ")\n",
    "\n",
    "epochs = 1\n",
    "model.fit(train_ds, epochs=epochs, validation_data=validation_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0fba2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
